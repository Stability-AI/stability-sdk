{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Using Stable Diffusion with AWS JumpStart](#toc0_)\n",
    "\n",
    "This sample notebook shows you how to deploy SDXL from Stability AI as an endpoint on Amazon SageMaker.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "\n",
    "## <a id='toc1_1_'></a>[Prerequisites](#toc0_)\n",
    "\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to <model_link>. If so, skip step: [Subscribe to the model package](#1.-Subscribe-to-the-model-package) <<<<<<<<< TODO >>>>>>>>>>\n",
    "\n",
    "\n",
    "## <a id='toc1_2_'></a>[Resources](#toc0_)\n",
    "\n",
    "\n",
    "1. [Stability SDK documentation](https://api.stability.ai/docs#tag/v1generation)\n",
    "\n",
    "2. [Documentation on real-time inference with Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html).\n",
    "\n",
    "\n",
    "## <a id='toc1_3_'></a>[Usage instructions](#toc0_)\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "- [1. Subscribe to the SDXL Model Package](#toc3_)    \n",
    "- [2: Create an endpoint and perform real-time inference](#toc4_)    \n",
    "  - [A: Text to image](#toc4_1_)    \n",
    "  - [B: Image to image](#toc4_2_)    \n",
    "- [3: Delete the endpoint](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[1. Subscribe to the SDXL Model Package](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the SDXL Model Package:\n",
    "1. Open the SDXL Model Package listing page <model_link>  <<<TODO>>>\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization accept the EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This identifies SDXL, and will be used to create your endpoint using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell. <<<TODO: check whether we need this>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import ModelPackage\n",
    "from stability_sdk_sagemaker.predictor import StabilityPredictor\n",
    "from stability_sdk_sagemaker.models import get_model_package_arn\n",
    "from stability_sdk.api import GenerationRequest,TextPrompt\n",
    "import boto3\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[2: Create an endpoint and perform real-time inference](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose your endpoint name\n",
    "endpoint_name='jumpstart-sdxl-stability-sdk-kate-rc2' # change this for each deployment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have subscribed to the Stability SDXL, pass in your Account ID and the Model Package name, `'stable-diffusion-xl-beta-v2-2-2-rc1'` to get the Model Package ARN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_arn = get_model_package_arn(model_package_name='stable-diffusion-xl-beta-v2-2-2-rc2', region_name='us-east-1', account_id='740929234339')\n",
    "role_arn = 'arn:aws:iam::740929234339:role/stability-api-sagemaker-execution-role-us-east-1'\n",
    "\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws sagemaker list-model-packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployable `ModelPackage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelPackage(role=role_arn,model_package_arn=package_arn,sagemaker_session=sagemaker_session,predictor_cls=StabilityPredictor)\n",
    "\n",
    "# Deploy the ModelPackage. This will take ~5 minutes to run\n",
    "# Instance type ml.g5.xlarge is sufficient for SDXL\n",
    "deployed_model = model.deploy(initial_instance_count=1,instance_type='ml.g5.xlarge',endpoint_name=endpoint_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already deployed your model, you can also access it via your chosen `endpoint_name` and `sagemaker_session`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deployed_model = StabilityPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `predict` on our deployed model to return model outputs. For the full list of parameters, [see the Stability.ai SDK documentation.](https://api.stability.ai/docs#tag/v1generation)\n",
    "\n",
    "## <a id='toc4_1_'></a>[A: Text to image](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"A photograph of fresh pizza with basil and tomatoes, from a traditional oven\")],\n",
    "                                             style_preset=\"cinematic\",\n",
    "                                             seed = 42\n",
    "                                             ))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output images are included in the response's `artifacts` as base64 encoded strings. Below is a helper function for accessing decoding these images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_show(model_response):\n",
    "    image = model_response.artifacts[0].base64\n",
    "    image_data = base64.b64decode(image.encode())\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    display(image)\n",
    "\n",
    "decode_and_show(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"teapot\")],\n",
    "                                             style_preset=\"origami\",\n",
    "                                             seed = 1234\n",
    "                                             ))\n",
    "decode_and_show(output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[B: Image to image](#toc0_)\n",
    "\n",
    "To perform inference that takes an image as input, you must pass in the image as `init_image` in the form of a base64-encoded string. Images must be of size (512, 512).\n",
    "\n",
    "Below is a helper function for converting images to base64-encoded strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"sun.png\"\n",
    "\n",
    "def encode_image(image_path, resize=True):\n",
    "    assert os.path.exists(image_path)\n",
    "\n",
    "    if resize:\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((512, 512))\n",
    "        image.save(\"image_path_resized.png\")\n",
    "        image_path = \"image_path_resized.png\"\n",
    "    image = Image.open(image_path)\n",
    "    assert image.size == (512, 512)\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        img_byte_array = image_file.read()\n",
    "        # Encode the byte array as a Base64 string\n",
    "        base64_str = base64.b64encode(img_byte_array).decode(\"utf-8\")\n",
    "    return base64_str\n",
    "\n",
    "img_base64_str = encode_image(image_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = deployed_model.predict(GenerationRequest(text_prompts=[TextPrompt(text=\"birds flying in front of the sun\")],\n",
    "                                             init_image=img_base64_str\n",
    "                                             ))\n",
    "\n",
    "decode_and_show(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[3: Delete the endpoint](#toc0_)\n",
    "\n",
    "When you've finished working, you can delete the endpoint to release the EC2 instance(s) associated with it, and stop billing.\n",
    "\n",
    "Get your list of Sagemaker endpoints using the AWS Sagemaker CLI like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws sagemaker list-endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete an endpoint\n",
    "deployed_model.sagemaker_session.delete_endpoint(endpoint_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, to delete the Model Package deployed to the endpoint, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
